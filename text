package etl

import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.expressions.Window
import java.sql.DriverManager
import java.util.Properties

object FullLeagueETL1 {

  // ---------- CONFIG ----------
  val jdbcUrl = "jdbc:postgresql://localhost:5432/FootballETL"
  val dbUser  = "postgres"
  val dbPass  = "root"
  val dbDriver = "org.postgresql.Driver"
  val targetSchema = "football_dw1"
  val keepLastNForForm = 5 // rolling form window

  val relevantLeagues = Set(
    "England Premier League",
    "France Ligue 1",
    "Germany 1. Bundesliga",
    "Italy Serie A",
    "Spain LIGA BBVA"
  )
  // ----------------------------

  private def ensureSchemaExists(): Unit = {
    Class.forName(dbDriver)
    val conn = DriverManager.getConnection(jdbcUrl, dbUser, dbPass)
    try {
      val stmt = conn.createStatement()
      stmt.execute(s"CREATE SCHEMA IF NOT EXISTS $targetSchema;")
      stmt.close()
      println(s"✅ Ensured schema: $targetSchema")
    } finally conn.close()
  }

  private def jdbcProps: Properties = {
    val p = new Properties()
    p.setProperty("user", dbUser)
    p.setProperty("password", dbPass)
    p.setProperty("driver", dbDriver)
    p
  }

  private def readTable(spark: SparkSession, tableName: String): DataFrame = {
    spark.read
      .jdbc(jdbcUrl, s""""public"."$tableName"""", jdbcProps)
      .persist()
  }

  private def writeTable(df: DataFrame, fullTableName: String): Unit = {
    df.write
      .mode(SaveMode.Overwrite)
      .jdbc(jdbcUrl, fullTableName, jdbcProps)
    println(s"✅ Wrote ${df.count()} rows to $fullTableName")
  }

  private def createIndexes(): Unit = {
    Class.forName(dbDriver)
    val conn = DriverManager.getConnection(jdbcUrl, dbUser, dbPass)
    try {
      val stmt = conn.createStatement()
      val idxs = Seq(
        s"CREATE INDEX IF NOT EXISTS idx_fact_match_league_season ON $targetSchema.fact_match (league_id, season);",
        s"CREATE INDEX IF NOT EXISTS idx_fact_match_home_team ON $targetSchema.fact_match (home_team_id);",
        s"CREATE INDEX IF NOT EXISTS idx_fact_match_away_team ON $targetSchema.fact_match (away_team_id);",
        s"CREATE INDEX IF NOT EXISTS idx_fact_team_season_team ON $targetSchema.fact_team_season (team_id, season);",
        s"CREATE INDEX IF NOT EXISTS idx_fact_league_season_league ON $targetSchema.fact_league_season (league_id, season);"
      )
      idxs.foreach(sql => stmt.execute(sql))
      stmt.close()
      println("✅ Created indexes on DW tables")
    } finally conn.close()
  }

  def cleanAndValidateData(
    spark: SparkSession,
    tCountry: DataFrame,
    tLeague: DataFrame,
    tTeam: DataFrame,
    tMatch: DataFrame,
    relevantLeagues: Set[String]
  ): (DataFrame, DataFrame, DataFrame, DataFrame) = {
    import spark.implicits._

    println("🔍 Starting data validation and cleaning...")

    val leaguesClean = tLeague
      .withColumn("name_clean", trim(lower(col("name"))))
      .filter(lower(col("name")).isin(relevantLeagues.map(_.toLowerCase).toSeq: _*))
      .select(
        col("id").cast("int").alias("league_id"),
        col("country_id").cast("int"),
        col("name_clean").alias("league_name")
      )
      .na.drop("any", Seq("league_id"))
      .dropDuplicates("league_id")

    val dupLeagues = leaguesClean.groupBy("league_id").count().filter($"count" > 1)
    if (dupLeagues.count() > 0) {
      println(s"⚠ Duplicate leagues found:")
      dupLeagues.show(false)
    }

    val countryIds = leaguesClean.select("country_id").distinct()
    val countriesClean = tCountry
      .withColumn("name_clean", trim(col("name")))
      .select(col("id").cast("int").alias("country_id"), col("name_clean").alias("country_name"))
      .join(countryIds, Seq("country_id"), "inner")
      .na.drop("any", Seq("country_id"))
      .dropDuplicates("country_id")

    val teamsClean = tTeam
      .withColumn("team_id", when(col("team_api_id").isNotNull, col("team_api_id")).otherwise(col("id")))
      .select(
        col("team_id").cast("int"),
        trim(col("team_long_name")).alias("team_name"),
        trim(col("team_short_name")).alias("team_short_name")
      )
      .na.drop("any", Seq("team_id"))
      .dropDuplicates("team_id")

    val matchesCleanRaw = tMatch
      .withColumn("league_id", col("league_id").cast("int"))
      .withColumn("home_team_id", col("home_team_api_id").cast("int"))
      .withColumn("away_team_id", col("away_team_api_id").cast("int"))
      .withColumn("home_team_goal", col("home_team_goal").cast("int"))
      .withColumn("away_team_goal", col("away_team_goal").cast("int"))
      .withColumn("date", to_timestamp(col("date")))
      .select("id", "league_id", "home_team_id", "away_team_id", "home_team_goal", "away_team_goal", "date")

    val validMatches = matchesCleanRaw
      .filter(col("league_id").isNotNull)
      .join(leaguesClean.select("league_id"), Seq("league_id"), "inner")
      .filter(col("home_team_id").isNotNull && col("away_team_id").isNotNull)
      .join(teamsClean.select("team_id"), matchesCleanRaw("home_team_id") === teamsClean("team_id"), "inner")
      .join(teamsClean.select("team_id"), matchesCleanRaw("away_team_id") === teamsClean("team_id"), "inner")
      .filter(col("home_team_goal").isNotNull && col("away_team_goal").isNotNull)
      .filter(col("date").isNotNull)
      .dropDuplicates("id")

    val badMatches = matchesCleanRaw.except(validMatches)
    if (badMatches.count() > 0) {
      println(s"⚠ Found ${badMatches.count()} invalid or unmatched match rows:")
      badMatches.show(10, false)
    }

    println("✅ Data cleaning and validation complete.")

    (countriesClean, leaguesClean, teamsClean, validMatches)
  }

  def run(spark: SparkSession): Unit = {
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    ensureSchemaExists()

    // === EXTRACT ===
    println("🔁 Extracting raw tables...")
    val tCountry = readTable(spark, "Country")
    val tLeague = readTable(spark, "League")
    val tTeam = readTable(spark, "Team")
    val tMatch = readTable(spark, "Match")
    val tPlayer = try { readTable(spark, "Player") } catch { case _: Throwable => spark.emptyDataFrame }
    val tPlayerAttr = try { readTable(spark, "Player_Attributes") } catch { case _: Throwable => spark.emptyDataFrame }
    val tTeamAttr = try { readTable(spark, "Team_Attributes") } catch { case _: Throwable => spark.emptyDataFrame }

    val (dimCountryFiltered, dimLeagueFiltered, dimTeamFiltered, factMatchFiltered) = 
      cleanAndValidateData(spark, tCountry, tLeague, tTeam, tMatch, relevantLeagues)

    println(s"Extracted & cleaned rows -> Country: ${dimCountryFiltered.count()}, League: ${dimLeagueFiltered.count()}, Team: ${dimTeamFiltered.count()}, Match: ${factMatchFiltered.count()}")

    // === FACT MATCH ENRICH ===
    val factMatchEnriched = factMatchFiltered
      .withColumn("result",
        when(col("home_team_goal") > col("away_team_goal"), lit("Home Win"))
          .when(col("home_team_goal") < col("away_team_goal"), lit("Away Win"))
          .otherwise(lit("Draw"))
      )
      .withColumn("goal_difference", col("home_team_goal") - col("away_team_goal"))
      .withColumn("home_points",
        when(col("home_team_goal") > col("away_team_goal"), lit(3))
          .when(col("home_team_goal") === col("away_team_goal"), lit(1))
          .otherwise(lit(0))
      )
      .withColumn("away_points",
        when(col("away_team_goal") > col("home_team_goal"), lit(3))
          .when(col("away_team_goal") === col("home_team_goal"), lit(1))
          .otherwise(lit(0))
      )
      .withColumn("season_start_year", regexp_extract(col("season"), """(\d{4})""", 1).cast(IntegerType))
      .withColumn("match_week", weekofyear(col("date")))
      .withColumn("match_date", to_date(col("date")))

    // === RELEVANT TEAMS (for filtering dim_team and players) ===
    val relevantTeamIds = factMatchEnriched
      .select("home_team_id").withColumnRenamed("home_team_id", "team_id")
      .union(factMatchEnriched.select(col("away_team_id").alias("team_id")))
      .distinct()

    val dimTeam = dimTeamFiltered
      .join(relevantTeamIds, Seq("team_id"), "inner")

    // === WRITE DIMENSIONS ===
    writeTable(dimCountryFiltered, s"$targetSchema.dim_country")
    writeTable(dimLeagueFiltered, s"$targetSchema.dim_league")
    writeTable(dimTeam, s"$targetSchema.dim_team")
    writeTable(factMatchEnriched, s"$targetSchema.fact_match")

    // === TEAM-SEASON AGGREGATES ===
    val homePerspective = factMatchEnriched.select(
      col("id").alias("match_id"), col("season"), col("league_id"), col("match_date"),
      col("home_team_id").alias("team_id"), col("away_team_id").alias("opponent_team_id"),
      col("home_team_goal").alias("goals_for"), col("away_team_goal").alias("goals_against"),
      col("home_points").alias("points")
    ).withColumn("is_home", lit(true))

    val awayPerspective = factMatchEnriched.select(
      col("id").alias("match_id"), col("season"), col("league_id"), col("match_date"),
      col("away_team_id").alias("team_id"), col("home_team_id").alias("opponent_team_id"),
      col("away_team_goal").alias("goals_for"), col("home_team_goal").alias("goals_against"),
      col("away_points").alias("points")
    ).withColumn("is_home", lit(false))

    val teamMatchPerspective = homePerspective.unionByName(awayPerspective)
      .filter(col("team_id").isNotNull)

    val factTeamSeason = teamMatchPerspective.groupBy("season", "league_id", "team_id")
      .agg(
        countDistinct("match_id").alias("matches_played"),
        sum(when(col("points") === 3, 1).otherwise(0)).alias("wins"),
        sum(when(col("points") === 1, 1).otherwise(0)).alias("draws"),
        sum(when(col("points") === 0, 1).otherwise(0)).alias("losses"),
        sum("goals_for").alias("goals_for"),
        sum("goals_against").alias("goals_against"),
        (sum("goals_for") - sum("goals_against")).alias("goal_difference"),
        sum("points").alias("points"),
        round(sum("points") / countDistinct("match_id"), 3).alias("win_rate")
      )

    writeTable(factTeamSeason, s"$targetSchema.fact_team_season")

    // === LEAGUE-SEASON AGGREGATES ===
    val factLeagueSeason = factTeamSeason.groupBy("season", "league_id")
      .agg(
        sum("matches_played").alias("total_matches"),
        round(avg("goals_for"), 2).alias("avg_goals_per_team"),
        round(avg("points"), 2).alias("avg_points_per_team"),
        max("points").alias("points_max")
      )

    writeTable(factLeagueSeason, s"$targetSchema.fact_league_season")

    // === ROLLING TEAM FORM ===
    val w = Window.partitionBy("team_id").orderBy(col("match_date").cast("timestamp")).rowsBetween(-keepLastNForForm, -1)
    val teamForm = teamMatchPerspective
      .withColumn("rolling_goals_for_lastN", sum("goals_for").over(w))
      .withColumn("rolling_goals_against_lastN", sum("goals_against").over(w))
      .withColumn("rolling_points_lastN", sum("points").over(w))
      .withColumn("form_matches", count("match_id").over(w))
      .select("team_id", "league_id", "season", "match_date", "form_matches", "rolling_points_lastN", "rolling_goals_for_lastN", "rolling_goals_against_lastN")
      .na.fill(0)

    writeTable(teamForm, s"$targetSchema.fact_team_form")

    // === HEAD-TO-HEAD AGGREGATES ===
    val pairDf = factMatchEnriched.select("id", "match_date", "home_team_id", "away_team_id", "home_team_goal", "away_team_goal")
      .withColumnRenamed("id", "match_id")
      .withColumn("team_a", least(col("home_team_id"), col("away_team_id")))
      .withColumn("team_b", greatest(col("home_team_id"), col("away_team_id")))
      .withColumn("a_goals", when(col("team_a") === col("home_team_id"), col("home_team_goal")).otherwise(col("away_team_goal")))
      .withColumn("b_goals", when(col("team_b") === col("away_team_id"), col("away_team_goal")).otherwise(col("home_team_goal")))

    val headToHead = pairDf.groupBy("team_a", "team_b")
      .agg(
        count("match_id").alias("meetings"),
        sum(when(col("a_goals") > col("b_goals"), 1).otherwise(0)).alias("a_wins"),
        sum(when(col("a_goals") === col("b_goals"), 1).otherwise(0)).alias("draws"),
        sum(when(col("a_goals") < col("b_goals"), 1).otherwise(0)).alias("b_wins"),
        sum("a_goals").alias("a_goals_total"),
        sum("b_goals").alias("b_goals_total"),
        round(sum(when(col("a_goals") > col("b_goals"), 1).otherwise(0)) / count("match_id"), 3).alias("a_win_rate"),
        round(sum(when(col("a_goals") < col("b_goals"), 1).otherwise(0)) / count("match_id"), 3).alias("b_win_rate")
      )

    writeTable(headToHead, s"$targetSchema.fact_head_to_head")

    // === PLAYER SEASON SNAPSHOT (only relevant players) ===
    if (!tPlayerAttr.rdd.isEmpty()) {
      // Note: Original player-team relationship logic may need adjustment for your source schema
      val relevantPlayers = tPlayer
        .select(col("player_api_id"), col("player_name"))
        .join(dimTeam.select("team_id"), tPlayer("player_api_id") === dimTeam("team_id"), "inner") 
        .select("player_api_id")
        .distinct()

      val playerAttr = tPlayerAttr
        .withColumn("date", to_date(col("date")))
        .withColumn("year", year(col("date")))
        .join(relevantPlayers, Seq("player_api_id"), "inner")

      val win = Window.partitionBy("player_api_id", "year").orderBy(col("date").desc)
      val playerYearSnapshot = playerAttr.withColumn("rn", row_number().over(win)).filter(col("rn") === 1).drop("rn")

      writeTable(
        playerYearSnapshot.select(
          col("player_api_id"),
          col("year"),
          col("overall_rating"),
          col("potential")
        ),
        s"$targetSchema.dim_player_year_snapshot"
      )
    }

    // === VALIDATIONS ===
    println("🔍 Running post-load validations...")
    val cntFact = spark.read.jdbc(jdbcUrl, s""""$targetSchema"."fact_match"""", jdbcProps).count()
    val cntTeamSeason = spark.read.jdbc(jdbcUrl, s""""$targetSchema"."fact_team_season"""", jdbcProps).count()
    println(s"Post-load counts: fact_match=$cntFact, fact_team_season=$cntTeamSeason")

    val factMatchDF = spark.read.jdbc(jdbcUrl, s""""$targetSchema"."fact_match"""", jdbcProps)
    val dimLeagueDF = spark.read.jdbc(jdbcUrl, s""""$targetSchema"."dim_league"""", jdbcProps)
    val missingLeagueFK = factMatchDF.join(dimLeagueDF, factMatchDF("league_id") === dimLeagueDF("league_id"), "left_anti")
    if (missingLeagueFK.count() > 0) {
      println(s"⚠ Warning: ${missingLeagueFK.count()} fact_match rows have league_id that don't exist in dim_league.")
    } else {
      println("✅ All league_ids in fact_match have matching entries in dim_league.")
    }

    val dupMatches = factMatchDF.groupBy("match_id").count().filter("count > 1")
    if (dupMatches.count() > 0) {
      println(s"⚠ Duplicate match_id found: ${dupMatches.count()} rows")
    } else {
      println("✅ No duplicate match_ids found.")
    }

    createIndexes()
    println("✅ Full ETL finished successfully.")
  }
}
